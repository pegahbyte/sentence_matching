{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating the Data\n",
    "## Simulation of Problem and Advice Pairs\n",
    "If match equals 1, the advice/answer belongs to the problem. If match equals 0, the answer does not respond to the problem.\n",
    "\n",
    "This is a small dummy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>advice</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I drink too much beer</td>\n",
       "      <td>Drink a non-alcoholic beverage between two dri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>Find friends with whom you can do the sport to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>Follow a strict diet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am always cold</td>\n",
       "      <td>Wear warmer cloths</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I cant focus</td>\n",
       "      <td>Try mindfulness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I feel I have a depression</td>\n",
       "      <td>A dog said yes once</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I drink too much beer</td>\n",
       "      <td>fish are nice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>sometimes it rains, sometimes the sun is out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>try to relax more often</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I cant focus</td>\n",
       "      <td>best tv show ever</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>china is a country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>japan is a country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I am not in a good shape. I don't feel fit</td>\n",
       "      <td>Follow a strict diet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I am freezing always</td>\n",
       "      <td>Wear warmer cloths</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I have anxiety</td>\n",
       "      <td>Try mindfulness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i dont do sports</td>\n",
       "      <td>find a fitness trainer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i have stress</td>\n",
       "      <td>try to relax more often</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I always feel nervousness.</td>\n",
       "      <td>try to relax more often</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>some singers are blond</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I cant focus</td>\n",
       "      <td>yesterday or tomorrow the stock market is unusual</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>flowers have many colors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       problem  \\\n",
       "0                        I drink too much beer   \n",
       "1                      I don't exercise enough   \n",
       "2                            I am overweighted   \n",
       "3                             I am always cold   \n",
       "4                                 I cant focus   \n",
       "5                   I feel I have a depression   \n",
       "6                        I drink too much beer   \n",
       "7                            I am overweighted   \n",
       "8                      I don't exercise enough   \n",
       "9                                 I cant focus   \n",
       "10                           I am overweighted   \n",
       "11                           I am overweighted   \n",
       "12  I am not in a good shape. I don't feel fit   \n",
       "13                        I am freezing always   \n",
       "14                              I have anxiety   \n",
       "15                            i dont do sports   \n",
       "16                               i have stress   \n",
       "17                 I always feel nervousness.    \n",
       "18                     I don't exercise enough   \n",
       "19                                I cant focus   \n",
       "20                           I am overweighted   \n",
       "\n",
       "                                               advice  match  \n",
       "0   Drink a non-alcoholic beverage between two dri...      1  \n",
       "1   Find friends with whom you can do the sport to...      1  \n",
       "2                                Follow a strict diet      1  \n",
       "3                                  Wear warmer cloths      1  \n",
       "4                                     Try mindfulness      1  \n",
       "5                                 A dog said yes once      0  \n",
       "6                                       fish are nice      0  \n",
       "7        sometimes it rains, sometimes the sun is out      0  \n",
       "8                             try to relax more often      0  \n",
       "9                                   best tv show ever      0  \n",
       "10                                 china is a country      0  \n",
       "11                                 japan is a country      0  \n",
       "12                               Follow a strict diet      1  \n",
       "13                                 Wear warmer cloths      1  \n",
       "14                                    Try mindfulness      1  \n",
       "15                             find a fitness trainer      1  \n",
       "16                            try to relax more often      1  \n",
       "17                            try to relax more often      1  \n",
       "18                             some singers are blond      0  \n",
       "19  yesterday or tomorrow the stock market is unusual      0  \n",
       "20                           flowers have many colors      0  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\"problem\":[\"I drink too much beer\", \"I don't exercise enough\"],\n",
    "     \"advice\":[\"Drink a non-alcoholic beverage between two drinks.\",\n",
    "               \"Find friends with whom you can do the sport together!\"]}\n",
    "test = pd.DataFrame(data=d)\n",
    "test[\"match\"]=1\n",
    "test.loc[2]=[\"I am overweighted\",\"Follow a strict diet\",1]\n",
    "test.loc[3]=[\"I am always cold\",\"Wear warmer cloths\",1]\n",
    "test.loc[4]=[\"I cant focus\",\"Try mindfulness\",1]\n",
    "test.loc[5]=[\"I feel I have a depression\",\"A dog said yes once\",0]\n",
    "test.loc[6]=[\"I drink too much beer\",\"fish are nice\",0]\n",
    "test.loc[7]=[\"I am overweighted\",\"sometimes it rains, sometimes the sun is out\",0]\n",
    "test.loc[8]=[\"I don't exercise enough\",\"try to relax more often\",0]\n",
    "test.loc[9]=[\"I cant focus\",\"best tv show ever\",0]\n",
    "test.loc[10]=[\"I am overweighted\",\"china is a country\",0]\n",
    "test.loc[11]=[\"I am overweighted\",\"japan is a country\",0]\n",
    "test.loc[12]=[\"I am not in a good shape. I don't feel fit\",\"Follow a strict diet\",1]\n",
    "test.loc[13]=[\"I am freezing always\",\"Wear warmer cloths\",1]\n",
    "test.loc[14]=[\"I have anxiety\",\"Try mindfulness\",1]\n",
    "test.loc[15]=[\"i dont do sports\",\"find a fitness trainer\",1]\n",
    "test.loc[16]=[\"i have stress\",\"try to relax more often\",1]\n",
    "test.loc[17]=[\"I always feel nervousness. \",\"try to relax more often\",1]\n",
    "test.loc[18]=[\"I don't exercise enough\",\"some singers are blond\",0]\n",
    "test.loc[19]=[\"I cant focus\",\"yesterday or tomorrow the stock market is unusual\",0]\n",
    "test.loc[20]=[\"I am overweighted\",\"flowers have many colors\",0]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Natural Language Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for splitting, deleting punctuation, word selection and stemming.\n",
    "\n",
    "Stemming not used here, but can be used if word embeddings are trained on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df, columns=[\"problem\", \"advice\"]):\n",
    "    '''This function creates two new columns for each input column.\n",
    "    One column without any punctuation or stop words and lower case,\n",
    "    and one with the split words in a list.\n",
    "    Additionally one columns with a combined list is created.'''\n",
    "    for column in columns:\n",
    "        df[column+\"_pure\"] = df[column].map(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "        #Making everything lowercase\n",
    "        df[column+\"_pure\"] = df[column+\"_pure\"].map(lambda x: x.lower())\n",
    "        #splitting the sentence into words\n",
    "        df[column+\"_pure\"] = df[column+\"_pure\"].map(lambda x: x.split())\n",
    "        #ps = PorterStemmer()\n",
    "        # delete stop words and stemming the remaining words\n",
    "        df[column+\"_pure\"] = df[column+\"_pure\"].map(lambda x:[word for word in x if not word in set(stopwords.words('english'))])\n",
    "        df[column+\"_pure\"] = df[column+\"_pure\"].map(lambda x: ' '.join(x))\n",
    "        df[column+\"_split\"]=df[column+\"_pure\"].apply(lambda x: x.split())\n",
    "        df[column+\"_split_nouns_and_verbs\"]=df[column+\"_pure\"].apply(lambda x: get_nouns_and_verbs(x))\n",
    "    df[\"both_split\"]=df.apply(lambda x: x.problem_split + x.advice_split, axis=1)\n",
    "    df[\"both_split_nouns_and_verbs\"]=df.apply(lambda x: x.problem_split_nouns_and_verbs + x.advice_split_nouns_and_verbs, axis=1)\n",
    "\n",
    "def stemming(df, columns=[\"problem\", \"advice\"]):\n",
    "    '''This function creates stemmed columns (similar as pre_processing function).'''\n",
    "    for column in columns:\n",
    "        df[column+\"_stemming\"] = df[column].map(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "        #Making everything lowercase\n",
    "        df[column+\"_stemming\"] = df[column+\"_stemming\"].map(lambda x: x.lower())\n",
    "        #splitting the sentence into words\n",
    "        df[column+\"_stemming\"] = df[column+\"_stemming\"].map(lambda x: x.split())\n",
    "        ps = PorterStemmer()\n",
    "        # delete stop words and stemming the remaining words\n",
    "        df[column+\"_stemming\"] = df[column+\"_stemming\"].map(lambda x:[ps.stem(word) for word in x if not word in set(stopwords.words('english'))])\n",
    "        df[column+\"_stemming\"] = df[column+\"_stemming\"].map(lambda x: ' '.join(x))\n",
    "        df[column+\"_split_stem\"]=df[column+\"_stemming\"].apply(lambda x: x.split())\n",
    "    #df[\"both_stem\"]=df.apply(lambda x: x.problem_split_stem + x.advice_split_stem, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns and Verbs only\n",
    "This function filters the text for nouns and verbs only. These word type seem the most useful for the task of understanding if an answer responds to a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns_and_verbs(lines):\n",
    "    '''This function return all nouns and verbs of a sentence'''\n",
    "    # function to test if something is a noun or verb (any tense)\n",
    "    is_noun_verb = lambda pos: pos[:2] in [\"NN\",\"VB\"]\n",
    "    # select nouns and verbs\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns_verbs = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun_verb(pos)] \n",
    "    return(nouns_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>advice</th>\n",
       "      <th>match</th>\n",
       "      <th>problem_pure</th>\n",
       "      <th>problem_split</th>\n",
       "      <th>problem_split_nouns_and_verbs</th>\n",
       "      <th>advice_pure</th>\n",
       "      <th>advice_split</th>\n",
       "      <th>advice_split_nouns_and_verbs</th>\n",
       "      <th>both_split</th>\n",
       "      <th>both_split_nouns_and_verbs</th>\n",
       "      <th>problem_stemming</th>\n",
       "      <th>problem_split_stem</th>\n",
       "      <th>advice_stemming</th>\n",
       "      <th>advice_split_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I drink too much beer</td>\n",
       "      <td>Drink a non-alcoholic beverage between two dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>drink much beer</td>\n",
       "      <td>[drink, much, beer]</td>\n",
       "      <td>[drink, beer]</td>\n",
       "      <td>drink non alcoholic beverage two drinks</td>\n",
       "      <td>[drink, non, alcoholic, beverage, two, drinks]</td>\n",
       "      <td>[drink, beverage, drinks]</td>\n",
       "      <td>[drink, much, beer, drink, non, alcoholic, bev...</td>\n",
       "      <td>[drink, beer, drink, beverage, drinks]</td>\n",
       "      <td>drink much beer</td>\n",
       "      <td>[drink, much, beer]</td>\n",
       "      <td>drink non alcohol beverag two drink</td>\n",
       "      <td>[drink, non, alcohol, beverag, two, drink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>Find friends with whom you can do the sport to...</td>\n",
       "      <td>1</td>\n",
       "      <td>exercise enough</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>find friends sport together</td>\n",
       "      <td>[find, friends, sport, together]</td>\n",
       "      <td>[find, friends, sport]</td>\n",
       "      <td>[exercise, enough, find, friends, sport, toget...</td>\n",
       "      <td>[exercise, enough, find, friends, sport]</td>\n",
       "      <td>exercis enough</td>\n",
       "      <td>[exercis, enough]</td>\n",
       "      <td>find friend sport togeth</td>\n",
       "      <td>[find, friend, sport, togeth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>Follow a strict diet</td>\n",
       "      <td>1</td>\n",
       "      <td>overweighted</td>\n",
       "      <td>[overweighted]</td>\n",
       "      <td>[overweighted]</td>\n",
       "      <td>follow strict diet</td>\n",
       "      <td>[follow, strict, diet]</td>\n",
       "      <td>[follow, diet]</td>\n",
       "      <td>[overweighted, follow, strict, diet]</td>\n",
       "      <td>[overweighted, follow, diet]</td>\n",
       "      <td>overweight</td>\n",
       "      <td>[overweight]</td>\n",
       "      <td>follow strict diet</td>\n",
       "      <td>[follow, strict, diet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   problem                                             advice  \\\n",
       "0    I drink too much beer  Drink a non-alcoholic beverage between two dri...   \n",
       "1  I don't exercise enough  Find friends with whom you can do the sport to...   \n",
       "2        I am overweighted                               Follow a strict diet   \n",
       "\n",
       "   match     problem_pure        problem_split problem_split_nouns_and_verbs  \\\n",
       "0      1  drink much beer  [drink, much, beer]                 [drink, beer]   \n",
       "1      1  exercise enough   [exercise, enough]            [exercise, enough]   \n",
       "2      1     overweighted       [overweighted]                [overweighted]   \n",
       "\n",
       "                               advice_pure  \\\n",
       "0  drink non alcoholic beverage two drinks   \n",
       "1              find friends sport together   \n",
       "2                       follow strict diet   \n",
       "\n",
       "                                     advice_split  \\\n",
       "0  [drink, non, alcoholic, beverage, two, drinks]   \n",
       "1                [find, friends, sport, together]   \n",
       "2                          [follow, strict, diet]   \n",
       "\n",
       "  advice_split_nouns_and_verbs  \\\n",
       "0    [drink, beverage, drinks]   \n",
       "1       [find, friends, sport]   \n",
       "2               [follow, diet]   \n",
       "\n",
       "                                          both_split  \\\n",
       "0  [drink, much, beer, drink, non, alcoholic, bev...   \n",
       "1  [exercise, enough, find, friends, sport, toget...   \n",
       "2               [overweighted, follow, strict, diet]   \n",
       "\n",
       "                 both_split_nouns_and_verbs problem_stemming  \\\n",
       "0    [drink, beer, drink, beverage, drinks]  drink much beer   \n",
       "1  [exercise, enough, find, friends, sport]   exercis enough   \n",
       "2              [overweighted, follow, diet]       overweight   \n",
       "\n",
       "    problem_split_stem                      advice_stemming  \\\n",
       "0  [drink, much, beer]  drink non alcohol beverag two drink   \n",
       "1    [exercis, enough]             find friend sport togeth   \n",
       "2         [overweight]                   follow strict diet   \n",
       "\n",
       "                            advice_split_stem  \n",
       "0  [drink, non, alcohol, beverag, two, drink]  \n",
       "1               [find, friend, sport, togeth]  \n",
       "2                      [follow, strict, diet]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = test\n",
    "columns = [\"problem\", \"advice\"]\n",
    "pre_processing(df, columns)\n",
    "stemming(df,columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "1. Calculating the word embeddings using pre-trained FastText word embedding (trained on wikpedia, vocabulary size of one million, and embedded into 300 dimensions.\n",
    "2. Substracting word-embedding vector for advice from word-embedding vector for problem elementwise\n",
    "3. Create features of the difference regarding every dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pegah\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "You have to download the ['wiki-news-300d-1M.vec'](https://fasttext.cc/docs/en/english-vectors.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pegah\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# This takes some time\n",
    "from gensim.models import KeyedVectors\n",
    "model2 = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sports', 0.8248535990715027), ('football', 0.6988055109977722), ('Sport', 0.6917921304702759), ('sport-', 0.6783909797668457), ('athletics', 0.675710916519165), ('sportsman', 0.6616963148117065), ('soccer', 0.6516904830932617), ('sporting', 0.6513649821281433), ('boxing', 0.6464983820915222), ('Sports', 0.6409319043159485)]\n"
     ]
    }
   ],
   "source": [
    "# Impressive example of word embedding\n",
    "# These are the words most similar to the word \"sport\"\n",
    "print(model2.most_similar('sport'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the \"distance\" between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sub(list1,list2):\n",
    "    '''This list subtracts list2 element-wise from list1\n",
    "    and return the difference c as a list.'''\n",
    "    try:\n",
    "        c = [a - b for a, b in zip(list1, list2)]\n",
    "    except:\n",
    "        pass\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Distances between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the word \"diet\" and the word \"nutrition\" is 25.140400115400553\n",
      "The distance between the word \"diet\" and the word \"stone\" is 34.320799998007715\n"
     ]
    }
   ],
   "source": [
    "a=list_sub(model2[\"diet\"],model2[\"nutrition\"])\n",
    "b=list_sub(model2[\"diet\"],model2[\"stone\"])\n",
    "c=map(abs, a)\n",
    "d=map(abs, b)\n",
    "print('The distance between the word \"diet\" and the word \"nutrition\" is', sum(c))\n",
    "print('The distance between the word \"diet\" and the word \"stone\" is', sum(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sentence embedding is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://ai.intelligentonlinetools.com/ml/text-vectors-word-embeddings-word2vec/\n",
    "def sent_vectorizer(sent, model):\n",
    "    '''This function averages the word embeddings (300 dimensions) of a sentence\n",
    "    and thus produces a sentence embedding (300 dimensions).'''\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent(df,model):\n",
    "    '''Creates sentence embeddings using all words.'''\n",
    "    df[\"avg_sentence_c_pt\"]=df[\"problem_split\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    df[\"avg_sentence_a_pt\"]=df[\"advice_split\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    \n",
    "def avg_sent_nv(df,model):\n",
    "    '''Creates sentence embedding using only nouns and verbs.'''\n",
    "    df[\"avg_sentence_c_pt_nv\"]=df[\"problem_split_nouns_and_verbs\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    df[\"avg_sentence_a_pt_nv\"]=df[\"advice_split_nouns_and_verbs\"].apply(lambda x: sent_vectorizer(x, model))\n",
    "    \n",
    "avg_sent(df,model2)\n",
    "avg_sent_nv(df,model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>advice</th>\n",
       "      <th>match</th>\n",
       "      <th>problem_pure</th>\n",
       "      <th>problem_split</th>\n",
       "      <th>problem_split_nouns_and_verbs</th>\n",
       "      <th>advice_pure</th>\n",
       "      <th>advice_split</th>\n",
       "      <th>advice_split_nouns_and_verbs</th>\n",
       "      <th>both_split</th>\n",
       "      <th>both_split_nouns_and_verbs</th>\n",
       "      <th>problem_stemming</th>\n",
       "      <th>problem_split_stem</th>\n",
       "      <th>advice_stemming</th>\n",
       "      <th>advice_split_stem</th>\n",
       "      <th>avg_sentence_c_pt</th>\n",
       "      <th>avg_sentence_a_pt</th>\n",
       "      <th>avg_sentence_c_pt_nv</th>\n",
       "      <th>avg_sentence_a_pt_nv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I drink too much beer</td>\n",
       "      <td>Drink a non-alcoholic beverage between two dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>drink much beer</td>\n",
       "      <td>[drink, much, beer]</td>\n",
       "      <td>[drink, beer]</td>\n",
       "      <td>drink non alcoholic beverage two drinks</td>\n",
       "      <td>[drink, non, alcoholic, beverage, two, drinks]</td>\n",
       "      <td>[drink, beverage, drinks]</td>\n",
       "      <td>[drink, much, beer, drink, non, alcoholic, bev...</td>\n",
       "      <td>[drink, beer, drink, beverage, drinks]</td>\n",
       "      <td>drink much beer</td>\n",
       "      <td>[drink, much, beer]</td>\n",
       "      <td>drink non alcohol beverag two drink</td>\n",
       "      <td>[drink, non, alcohol, beverag, two, drink]</td>\n",
       "      <td>[-0.0791, -0.06826667, 0.027566666, 0.02183333...</td>\n",
       "      <td>[-0.016916666, -0.065866664, 0.021750003, 0.06...</td>\n",
       "      <td>[-0.095649995, -0.086399995, 0.008300001, 0.05...</td>\n",
       "      <td>[-0.036166668, -0.031600002, 0.05066667, 0.049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>Find friends with whom you can do the sport to...</td>\n",
       "      <td>1</td>\n",
       "      <td>exercise enough</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>find friends sport together</td>\n",
       "      <td>[find, friends, sport, together]</td>\n",
       "      <td>[find, friends, sport]</td>\n",
       "      <td>[exercise, enough, find, friends, sport, toget...</td>\n",
       "      <td>[exercise, enough, find, friends, sport]</td>\n",
       "      <td>exercis enough</td>\n",
       "      <td>[exercis, enough]</td>\n",
       "      <td>find friend sport togeth</td>\n",
       "      <td>[find, friend, sport, togeth]</td>\n",
       "      <td>[-0.084750004, -0.052950002, -0.09095, 0.0191,...</td>\n",
       "      <td>[-0.061075002, -0.0038499986, 0.043325003, 0.0...</td>\n",
       "      <td>[-0.084750004, -0.052950002, -0.09095, 0.0191,...</td>\n",
       "      <td>[-0.091066666, -0.0054666647, 0.05796667, 0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   problem                                             advice  \\\n",
       "0    I drink too much beer  Drink a non-alcoholic beverage between two dri...   \n",
       "1  I don't exercise enough  Find friends with whom you can do the sport to...   \n",
       "\n",
       "   match     problem_pure        problem_split problem_split_nouns_and_verbs  \\\n",
       "0      1  drink much beer  [drink, much, beer]                 [drink, beer]   \n",
       "1      1  exercise enough   [exercise, enough]            [exercise, enough]   \n",
       "\n",
       "                               advice_pure  \\\n",
       "0  drink non alcoholic beverage two drinks   \n",
       "1              find friends sport together   \n",
       "\n",
       "                                     advice_split  \\\n",
       "0  [drink, non, alcoholic, beverage, two, drinks]   \n",
       "1                [find, friends, sport, together]   \n",
       "\n",
       "  advice_split_nouns_and_verbs  \\\n",
       "0    [drink, beverage, drinks]   \n",
       "1       [find, friends, sport]   \n",
       "\n",
       "                                          both_split  \\\n",
       "0  [drink, much, beer, drink, non, alcoholic, bev...   \n",
       "1  [exercise, enough, find, friends, sport, toget...   \n",
       "\n",
       "                 both_split_nouns_and_verbs problem_stemming  \\\n",
       "0    [drink, beer, drink, beverage, drinks]  drink much beer   \n",
       "1  [exercise, enough, find, friends, sport]   exercis enough   \n",
       "\n",
       "    problem_split_stem                      advice_stemming  \\\n",
       "0  [drink, much, beer]  drink non alcohol beverag two drink   \n",
       "1    [exercis, enough]             find friend sport togeth   \n",
       "\n",
       "                            advice_split_stem  \\\n",
       "0  [drink, non, alcohol, beverag, two, drink]   \n",
       "1               [find, friend, sport, togeth]   \n",
       "\n",
       "                                   avg_sentence_c_pt  \\\n",
       "0  [-0.0791, -0.06826667, 0.027566666, 0.02183333...   \n",
       "1  [-0.084750004, -0.052950002, -0.09095, 0.0191,...   \n",
       "\n",
       "                                   avg_sentence_a_pt  \\\n",
       "0  [-0.016916666, -0.065866664, 0.021750003, 0.06...   \n",
       "1  [-0.061075002, -0.0038499986, 0.043325003, 0.0...   \n",
       "\n",
       "                                avg_sentence_c_pt_nv  \\\n",
       "0  [-0.095649995, -0.086399995, 0.008300001, 0.05...   \n",
       "1  [-0.084750004, -0.052950002, -0.09095, 0.0191,...   \n",
       "\n",
       "                                avg_sentence_a_pt_nv  \n",
       "0  [-0.036166668, -0.031600002, 0.05066667, 0.049...  \n",
       "1  [-0.091066666, -0.0054666647, 0.05796667, 0.02...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creator(df,size=300,only_noun_and_verbs=True):\n",
    "    '''This function calculates the differences in sentence embeddings\n",
    "    elementwise and returns the dataframe with additional 300 features.'''\n",
    "    dfx=df.copy(deep=False)\n",
    "    # if PreTrained\n",
    "    suffixes=[\"pt\"]\n",
    "    for suffix in suffixes:\n",
    "        #df['problem_'+suffix] = \n",
    "        #df['advice_'+suffix] = \n",
    "        if only_noun_and_verbs==True:\n",
    "            dfx[\"distance_\"+suffix+\"_nv\"] = dfx.apply(lambda x: list_sub(x['avg_sentence_c_'+suffix+\"_nv\"],x['avg_sentence_a_'+suffix+\"_nv\"]), axis=1)\n",
    "        else:\n",
    "            dfx[\"distance_\"+suffix] = dfx.apply(lambda x: list_sub(x['avg_sentence_c_'+suffix],x['avg_sentence_a_'+suffix]), axis=1)\n",
    "        #Adding new columns, a column for each dimension of the differences-vector\n",
    "        #This way is not working - find another way\n",
    "        #if suffix==\"t\":\n",
    "        #    j=1000\n",
    "        #else:\n",
    "        #    j=0\n",
    "        #for i in range(j,size+j):\n",
    "        if only_noun_and_verbs==True:\n",
    "            for i in range(size):\n",
    "                dfx[str(i)] = dfx[\"distance_\"+suffix+\"_nv\"].apply(lambda x : x[i])\n",
    "        else:\n",
    "            for i in range(size):\n",
    "                dfx[str(i)] = dfx[\"distance_\"+suffix].apply(lambda x : x[i])\n",
    "    return(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>advice</th>\n",
       "      <th>match</th>\n",
       "      <th>problem_pure</th>\n",
       "      <th>problem_split</th>\n",
       "      <th>problem_split_nouns_and_verbs</th>\n",
       "      <th>advice_pure</th>\n",
       "      <th>advice_split</th>\n",
       "      <th>advice_split_nouns_and_verbs</th>\n",
       "      <th>both_split</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I drink too much beer</td>\n",
       "      <td>Drink a non-alcoholic beverage between two dri...</td>\n",
       "      <td>1</td>\n",
       "      <td>drink much beer</td>\n",
       "      <td>[drink, much, beer]</td>\n",
       "      <td>[drink, beer]</td>\n",
       "      <td>drink non alcoholic beverage two drinks</td>\n",
       "      <td>[drink, non, alcoholic, beverage, two, drinks]</td>\n",
       "      <td>[drink, beverage, drinks]</td>\n",
       "      <td>[drink, much, beer, drink, non, alcoholic, bev...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00585</td>\n",
       "      <td>-0.023767</td>\n",
       "      <td>-0.018083</td>\n",
       "      <td>0.095133</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>-0.009617</td>\n",
       "      <td>-0.05420</td>\n",
       "      <td>0.042933</td>\n",
       "      <td>0.063250</td>\n",
       "      <td>-0.052583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't exercise enough</td>\n",
       "      <td>Find friends with whom you can do the sport to...</td>\n",
       "      <td>1</td>\n",
       "      <td>exercise enough</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>[exercise, enough]</td>\n",
       "      <td>find friends sport together</td>\n",
       "      <td>[find, friends, sport, together]</td>\n",
       "      <td>[find, friends, sport]</td>\n",
       "      <td>[exercise, enough, find, friends, sport, toget...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10530</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>-0.011767</td>\n",
       "      <td>-0.023933</td>\n",
       "      <td>-0.053433</td>\n",
       "      <td>0.030083</td>\n",
       "      <td>0.08210</td>\n",
       "      <td>0.090517</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>-0.087217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am overweighted</td>\n",
       "      <td>Follow a strict diet</td>\n",
       "      <td>1</td>\n",
       "      <td>overweighted</td>\n",
       "      <td>[overweighted]</td>\n",
       "      <td>[overweighted]</td>\n",
       "      <td>follow strict diet</td>\n",
       "      <td>[follow, strict, diet]</td>\n",
       "      <td>[follow, diet]</td>\n",
       "      <td>[overweighted, follow, strict, diet]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14345</td>\n",
       "      <td>-0.067350</td>\n",
       "      <td>-0.110650</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>-0.053350</td>\n",
       "      <td>-0.191450</td>\n",
       "      <td>0.10595</td>\n",
       "      <td>-0.035050</td>\n",
       "      <td>-0.083550</td>\n",
       "      <td>-0.023550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   problem                                             advice  \\\n",
       "0    I drink too much beer  Drink a non-alcoholic beverage between two dri...   \n",
       "1  I don't exercise enough  Find friends with whom you can do the sport to...   \n",
       "2        I am overweighted                               Follow a strict diet   \n",
       "\n",
       "   match     problem_pure        problem_split problem_split_nouns_and_verbs  \\\n",
       "0      1  drink much beer  [drink, much, beer]                 [drink, beer]   \n",
       "1      1  exercise enough   [exercise, enough]            [exercise, enough]   \n",
       "2      1     overweighted       [overweighted]                [overweighted]   \n",
       "\n",
       "                               advice_pure  \\\n",
       "0  drink non alcoholic beverage two drinks   \n",
       "1              find friends sport together   \n",
       "2                       follow strict diet   \n",
       "\n",
       "                                     advice_split  \\\n",
       "0  [drink, non, alcoholic, beverage, two, drinks]   \n",
       "1                [find, friends, sport, together]   \n",
       "2                          [follow, strict, diet]   \n",
       "\n",
       "  advice_split_nouns_and_verbs  \\\n",
       "0    [drink, beverage, drinks]   \n",
       "1       [find, friends, sport]   \n",
       "2               [follow, diet]   \n",
       "\n",
       "                                          both_split    ...         290  \\\n",
       "0  [drink, much, beer, drink, non, alcoholic, bev...    ...     0.00585   \n",
       "1  [exercise, enough, find, friends, sport, toget...    ...    -0.10530   \n",
       "2               [overweighted, follow, strict, diet]    ...     0.14345   \n",
       "\n",
       "        291       292       293       294       295      296       297  \\\n",
       "0 -0.023767 -0.018083  0.095133  0.089700 -0.009617 -0.05420  0.042933   \n",
       "1  0.013617 -0.011767 -0.023933 -0.053433  0.030083  0.08210  0.090517   \n",
       "2 -0.067350 -0.110650  0.067650 -0.053350 -0.191450  0.10595 -0.035050   \n",
       "\n",
       "        298       299  \n",
       "0  0.063250 -0.052583  \n",
       "1 -0.003083 -0.087217  \n",
       "2 -0.083550 -0.023550  \n",
       "\n",
       "[3 rows x 320 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = feature_creator(df,size=300)\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['problem', 'advice', 'match', 'problem_pure', 'problem_split',\n",
      "       'problem_split_nouns_and_verbs', 'advice_pure', 'advice_split',\n",
      "       'advice_split_nouns_and_verbs', 'both_split',\n",
      "       'both_split_nouns_and_verbs', 'problem_stemming', 'problem_split_stem',\n",
      "       'advice_stemming', 'advice_split_stem', 'avg_sentence_c_pt',\n",
      "       'avg_sentence_a_pt', 'avg_sentence_c_pt_nv', 'avg_sentence_a_pt_nv',\n",
      "       'distance_pt_nv', '0', '1', '2', '3', '4'],\n",
      "      dtype='object')\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
      "       '13', '14'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Features start from column 20\n",
    "print(df2.columns[:25])\n",
    "print(df2.columns[20:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def match_pred(dfx, seed):\n",
    "    y=dfx[\"match\"]\n",
    "    # The word embedding features start at 20th columns (not a robust solution here though)\n",
    "    X=dfx.iloc[:,20:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = seed)\n",
    "    log = LogisticRegression(solver='liblinear')#regularization is applied by default\n",
    "    log.fit(X_train, y_train)\n",
    "    accuracy=log.score(X_test,y_test)\n",
    "    y_pred = log.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #print(cm)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average test accuracy for 1000 random train-test-splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null rate\n",
    "df.match.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Only using nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=[]\n",
    "for i in range(1000):\n",
    "    acc_i = match_pred(df2,i)\n",
    "    acc.append(acc_i)\n",
    "mean_acc=sum(acc)/len(acc)\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Using all word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = feature_creator(df,size=300,only_noun_and_verbs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040909090909102"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=[]\n",
    "for i in range(1000):\n",
    "    acc_i = match_pred(df3,i)\n",
    "    acc.append(acc_i)\n",
    "mean_acc=sum(acc)/len(acc)\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Given the extremely small data set (n=20) and the short sentences, this is a promising first result. The null rate is 52 % and we can see an improvement of around 8 percentage points.\n",
    "\n",
    "In this very small case, using all word types is slightly superior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
